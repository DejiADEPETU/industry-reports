import React from 'react'
import '../styles/Home.css';
    export default function Home() {
        return (
            <React.Fragment>
                <div className="home-container">
                        
                <div>
                    <h3>
                    INTRODUCTION
                    </h3>
                </div>

                <div>
                    <p>
                    Can international human rights help guide and govern artificial intelligence (AI)?
                    According to the global ethics initiative of the Institute of Electrical and Electronics
                    Engineers (IEEE), the largest organization of technical professionals, the answer is
                    clear. The IEEE’s 2017 report on ethically aligned design for AI lists as its first principle
                    that AI design should not infringe upon international human rights.1

                    Yet some AI sys-
                    tems are already infringing on such rights. For instance, in March 2018, human rights

                    investigators from the United Nations (UN) found that Facebook – and its algorithmi-
                    cally driven news feed – exacerbated the circulation of hate speech and incitement to

                    violence in Myanmar.2 During a US Congressional hearing in April 2018, Senator Pat-
                    rick Leahy questioned CEO Mark Zuckerberg about the failure of Facebook’s AI for

                    content detection in the face of possible genocide against Myanmar’s Rohingya ethic
                    minority. While Zuckerberg initially told senators that more advanced AI tools would
                    help solve the problem, he later conceded to investors that Facebook’s AI systems will
                    be unable to detect “hate” in local contexts with reasonable accuracy anytime soon.3
                    Just a month after Zuckerberg’s hearing, the UN’s International Telecommunications
                    Union (ITU) hosted its second annual AI for Global Good summit in Geneva.4 For many
                    involved in the summit, AI is not just a source of potential risks, it can bring a better
                    future of worldwide benefits. Between these hopes and fears lies an increased sense
                    of uncertainty. As stakeholders look for a North Star to guide AI development, we can
                    rely on human rights to help chart the course ahead. Simply put:
                    </p>
                </div>

                <div className="ConclusionImage1">
                <img className="imageCon1" src="https://www.canterbury.ac.nz/engineering/schools/csse/research/ai/Brain-image-on-a-circuit-board_4014951147079649991.jpg" height="200px"/>
                </div>
                

                <div>
                    <p>
                    In order for AI to benefit the common good, at the very least its
                    design and deployment should avoid harms to fundamental human
                    values. International human rights provide a robust and global
                    formulation of those values.

                    </p>
                </div>

                <div>
                    <p>
                    In bridging AI and human rights, what’s at stake is human dignity.* As an international
                    framework, human rights law is intended to establish global principles (“norms”) and
                    mechanisms of accountability for the treatment of individuals. As such, a rights-based
                    </p>
                </div>
        
                <div>
                    <h4>
                    GOVERNING ARTIFICIAL INTELLIGENCE
                    </h4>               
                </div>

                <div>
                    <p>
                    approach provides actors developing AI with the aspirational and normative guid-
                    ance to uphold human dignity and the inherent worth of every individual, regardless

                    of country or jurisdiction. Implementing human rights can help identify and anticipate

                    some of AI’s worst social harms and guide those developing technical and policy safe-
                    guards to promote positive uses. Those working on AI accountability can activate the

                    international system of human rights practice – including binding treaties, UN investi-
                    gations, and advocacy initiatives – to monitor social impacts and establish processes

                    of redress. Importantly, advocates can use human rights to focus attention on power
                    relationships and inequalities that impact vulnerable or marginalized groups around
                    the globe.
                    </p>
                </div>
        

                <div>
                    <h2>
                    IMPLEMENTING HUMAN RIGHTS CAN HELP
                    IDENTIFY AND ANTICIPATE SOME OF AI’S WORST
                    SOCIAL HARMS AND GUIDE THOSE DEVELOPING
                    TECHNICAL AND POLICY SAFEGUARDS TO
                    PROMOTE POSITIVE USES.
                    </h2>
                </div>

                <div className="ConclusionImage1">
                <img className="imageCon1" src="https://news.blrstage.com/app/uploads/sites/3/2018/11/AI-Machine-Learning-5.jpg" height="200px"/>
                </div>
            
        

                <div>
                    <p>
                    Those working on AI commercially might wonder why they should care about human

                    rights. Increasingly, stakeholders are holding the private sector responsible for up-
                    holding rights.5 In 2011, the UN released a landmark document – The Guiding Princi-
                    ples on Business and Human Rights – that calls on industry to respect, protect, and

                    provide remedies for human rights.6 These principles can provide AI executives and

                    developers alike with a template for conducting due diligence on human rights im-
                    pacts. They provide guidelines for how businesses should assume a higher duty of

                    care when developing and deploying their products.7

                    Although a milestone in the field of business and human rights, the UN Guiding Princi-
                    ples reflects but a starting point for the application of human rights in the tech sector.

                    Those working directly on AI need regulation, or “hard” laws, along with technical
                    standards, social norms, and market incentives, to effectively incorporate a respect
                    for human rights into their business models, policies, and practices. At the same time,
                    those working on human rights need to be actively engaged in AI governance and

                    monitoring. When necessary, they should be ready to invoke a human rights frame-
                    work to challenge how AI is developed and deployed by business or government. Civil

                    society and AI developers should work together to help assess risk areas and antici-
                    pate the needs of vulnerable groups. Only when stakeholders are working across silos

                    to safeguard against harms can AI systems avoid human rights abuses and advance
                    the enjoyment of human rights.

                    This report is intended as a resource for anyone working in the field of AI and gover-
                    nance. Anywhere that AI is being researched, developed, or deployed, a human rights

                    frame can identify, anticipate, and minimize an important class of risks and harms. This
                    work is also intended for those in the human rights field, outlining why they should be
                    concerned about the present and near-term impacts of AI. What follows translates
                    between these fields by reframing the societal impact of AI systems through the lens
                    of human rights.
                    For those seeking to govern AI – from governments looking to craft regulation to
                    companies looking to self-regulate – this document offers a perspective based on

                    established human rights accountability and norms. The field of human rights has lim-
                    itations and will certainly not address all the ethical issues arising from AI. Yet it offers

                    a strong value proposition: an approach to AI governance that upholds human dignity
                    based on international human rights law.
                    The first part of this report, “Bridging AI and Human Rights,” connects the entry points
                    between AI and human rights for governance discussions. Next, “A Human Rights
                    Frame for AI Risks and Harms” reviews a number of current AI risks and harms from
                    a human rights perspective, describing how such rights can be applied. Part three,
                    “Stakeholder Overview,” catalogues the current state of play among stakeholders
                    active in this space, with examples of progress and challenges. Finally, the conclusion
                    discusses the limitations and presents several recommendations for incorporating a
                    human rights approach for AI governance.
                        </p>
                    </div>
        

            <h3>
            BRIDGING AI AND HUMAN RIGHTS
            </h3>


            <div>
                <p>
            Human rights have only appeared at the periphery of our prominent AI debates.8 Both
            AI and human rights are highly technical fields; to fully digest either would require far
            more of an exegesis than can be attempted in this report. Instead, we shall draw on
            basic entry points from both fields to inform AI governance discussions.
            Discussions about AI can be fragmented; some people speak of AI colloquially in
            the popular press or in tech marketing materials, while others speak of concrete
            methods in scientific proceedings.9 Moreover, the nuances of terminology and the
            speed at which the field is moving can make cross-disciplinary discussions difficult to
            have. When considering social and policy implications, it is useful to think of “AI” as a
            catchphrase for a cluster of technologies embedded in social systems. This includes
            machine learning, natural language processing, computer vision, neural networks,
            deep learning, big data analytics, predictive models, algorithms, and robotics—all of
            which are intrinsically situated in the social contexts where they are developed and
            deployed.
            While some areas of AI remain only theoretical, others, such as machine learning, are
            already having an impact in real-world contexts.10 Machine learning systems process
            large amounts of historical training data and “learn” from these examples to detect
            patterns that can be useful in decision-making.11 All machine learning algorithms
            contain some level of statistical bias, which produces incorrect decisions some of
            the time.12 However, if the historical data are incomplete or are not representative
            of a specified population, these biases can scale quickly and inexplicably across 
            AI systems. Such systems can further entrench discriminatory outcomes in people’s lives.
            </p>

            <div className="ConclusionImage1">
                <img className="imageCon1" src="https://h2q8k3x8.stackpathcdn.com/wp-content/uploads/2017/05/artificial-intelligence-robot-ai.jpg" height="200px"/>
            </div>


            <p>
            


                    How far should we as a society allow machine learning systems to influence human
                    decision-making or even make decisions on their own? These concerns are at the
                    heart of AI debates.13
                    While these questions have yet to be answered, the fact is that today, automated
                    systems are making predictions about human behavior and producing decisions and
                    recommendations that are impacting people in everyday life. These systems are
                    increasingly becoming embedded in a number of social contexts, from policing and
                    judicial sentencing to medicine and finance. We do not know the unintentional impacts
                    or unforeseen consequences of current or future AI systems. As this uncertainty has
                    brought urgent calls to govern AI, we can now turn to the value of human rights.
                    The field of human rights can be complex for nonexperts. For the purposes of this
                    report, we anchor international human rights law in the drafting and implementation
                    of the Universal Declaration on Human Rights (UDHR) by the United Nations in 1948.
                    The UDHR’s aspirational language established that human rights were grounded in a

                    respect for all individuals that derived from our equal status as bearers of inherent human dignity. This was a response to the “disregard and contempt for human rights,”14

                    which precipitated two world wars and the Holocaust. Human dignity and fundamen-
                    tal rights are not tied to country citizenship, legal regime, or socioeconomic position.

                    These rights are universal in the sense that they apply to everyone, everywhere, which
                    provides a frame for discussing global AI impact and governance.
                    Over the last 70 years, human rights proponents have developed the principles of the
                    UDHR into a body of international human rights law that includes nine major human

                    rights treaties; regional rights instruments in the Americas, Africa, and Europe; incor-
                    poration in state constitutions and national laws; and customary and case law.15 Yet

                    because of a divergence in political ideologies and claims to sovereignty, governments
                    enforce international human rights law to wildly varying degrees.16 Thus, a human
                    rights framework has emerged to monitor, promote, and protect human rights. This
                    involves the further development of international human rights law and the interaction
                    of a diverse network of actors in the UN system, nation-states, international organiza-
                    tions, NGOs, civil society, the private sector, academia, and advocates at the local or

                    individual level.
                    Those looking for first principles to ground AI governance can use the language of

                    human rights. For example, one of the most hotly debated topics in AI is discrimina-
                    tory algorithms and systems. This includes empirical research on facial recognition

                    systems that cannot “see” people, particularly women, with darker skin due to a lack

                    of adequate training data or to faulty models and therefore reproduce culturally en-
                    grained biases against people of color.17 Human rights principles of nondiscrimination

                    have been propagated through a multitude of UN treaties, national laws, UN commen-
                    tary, academic interpretation, and other policies and guidelines. This body of work

                    offers not only a distinct value commitment but also a global perspective on how to
                    identify the impact of discrimination. Equality and nondiscrimination are foundational
                    to practically every major human rights instrument that has emerged from debate and
                    input from representatives from the world’s nations. The development of human rights
                    has its own controversies and politics, but over the last 70 years, international human
                    rights have come to represent shared global values.
                    Those working on technology policy are faced with the difficult task of deciding what
                    standards, values, or norms to apply in different social contexts. They need to balance
                    the tradeoffs of developing or deploying technologies. They need to understand the
                    potential misuses and abuses, unintended consequences, biases in sociotechnical
                    systems, and even the costs of not deploying a tool when it may help someone in
                    
                    need. Human rights provide those working on AI with a basis for understanding why
                    governing systems – from technical standards to policy – should address values like

                    nondiscrimination in the first place. This is important for tech companies whose prod-
                    ucts will be used across national borders where laws and values vary.

                    While it is outside the present scope of this report, an area that demands more foun-
                    dational work concerns pathways for human rights accountability and remedy when

                    AI harms become manifest. A purely legal, regulatory, or compliance framework would
                    lag behind the velocity of change associated with emerging AI technologies. Thus,
                    other components of the human rights framework, such as UN special rapporteurs,
                    independent investigators, and monitors from civil society are crucial for calling

                    attention to AI risks and harms. As scholars Christiaan Van Veen of New York Univer-
                    sity (NYU) and Corinne Cath of Oxford Internet Institute state, “Human rights, as a

                    language and legal framework, is itself a source of power because human rights carry
                    significant moral legitimacy and the reputational cost of being perceived as a human
                    rights violator can be very high.”18 Thus, human rights can provide the link between an
                    AI system’s negative social impact on even one individual in places like Myanmar and
                    the most powerful companies in Silicon Valley.
                </p>
            </div>
            </div>
        </React.Fragment>
    )
}
