{"ast":null,"code":"var _jsxFileName = \"/Users/Student_M7/Desktop/SE/myGitHub/industry-reports/industry-reports/src/components/Home.js\";\nimport React from 'react';\nexport default function Home() {\n  return /*#__PURE__*/React.createElement(\"div\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 5,\n      columnNumber: 9\n    }\n  }, /*#__PURE__*/React.createElement(\"p\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 9,\n      columnNumber: 13\n    }\n  }, \"INTRODUCTION Can international human rights help guide and govern artificial intelligence (AI)? According to the global ethics initiative of the Institute of Electrical and Electronics Engineers (IEEE), the largest organization of technical professionals, the answer is clear. The IEEE\\u2019s 2017 report on ethically aligned design for AI lists as its first principle that AI design should not infringe upon international human rights.1 Yet some AI sys- tems are already infringing on such rights. For instance, in March 2018, human rights investigators from the United Nations (UN) found that Facebook \\u2013 and its algorithmi- cally driven news feed \\u2013 exacerbated the circulation of hate speech and incitement to violence in Myanmar.2 During a US Congressional hearing in April 2018, Senator Pat- rick Leahy questioned CEO Mark Zuckerberg about the failure of Facebook\\u2019s AI for content detection in the face of possible genocide against Myanmar\\u2019s Rohingya ethic minority. While Zuckerberg initially told senators that more advanced AI tools would help solve the problem, he later conceded to investors that Facebook\\u2019s AI systems will be unable to detect \\u201Chate\\u201D in local contexts with reasonable accuracy anytime soon.3 Just a month after Zuckerberg\\u2019s hearing, the UN\\u2019s International Telecommunications Union (ITU) hosted its second annual AI for Global Good summit in Geneva.4 For many involved in the summit, AI is not just a source of potential risks, it can bring a better future of worldwide benefits. Between these hopes and fears lies an increased sense of uncertainty. As stakeholders look for a North Star to guide AI development, we can rely on human rights to help chart the course ahead. Simply put: In order for AI to benefit the common good, at the very least its design and deployment should avoid harms to fundamental human values. International human rights provide a robust and global formulation of those values. In bridging AI and human rights, what\\u2019s at stake is human dignity.* As an international framework, human rights law is intended to establish global principles (\\u201Cnorms\\u201D) and mechanisms of accountability for the treatment of individuals. As such, a rights-based * The definition of human dignity is contested and its normative value is debated in an extensive literature that is outside the scope of this report. For the present purposes, the term human dignity gestures towards its usage in Western moral philosophy, such as Kant\\u2019s notions of dignity linked to human autonomy and agency, while acknowledging that dignity has been linked to traditions such as Eastern philosophy as well. This report\\u2019s usage of human dignity also evokes the United Nation\\u2019s charter, the Universal Declaration on Human Rights, and the major rights treaties, which link fundamental human rights, the dignity and worth of the human person, and the equal rights of men and women. The interactions between humans and AI may further challenge or refine the concept of human dignity, which is an important topic for future work. DATA & SOCIETY 6 GOVERNING ARTIFICIAL INTELLIGENCE approach provides actors developing AI with the aspirational and normative guid- ance to uphold human dignity and the inherent worth of every individual, regardless of country or jurisdiction. Implementing human rights can help identify and anticipate some of AI\\u2019s worst social harms and guide those developing technical and policy safe- guards to promote positive uses. Those working on AI accountability can activate the international system of human rights practice \\u2013 including binding treaties, UN investi- gations, and advocacy initiatives \\u2013 to monitor social impacts and establish processes of redress. Importantly, advocates can use human rights to focus attention on power relationships and inequalities that impact vulnerable or marginalized groups around the globe. IMPLEMENTING HUMAN RIGHTS CAN HELP IDENTIFY AND ANTICIPATE SOME OF AI\\u2019S WORST SOCIAL HARMS AND GUIDE THOSE DEVELOPING TECHNICAL AND POLICY SAFEGUARDS TO PROMOTE POSITIVE USES. Those working on AI commercially might wonder why they should care about human rights. Increasingly, stakeholders are holding the private sector responsible for up- holding rights.5 In 2011, the UN released a landmark document \\u2013 The Guiding Princi- ples on Business and Human Rights \\u2013 that calls on industry to respect, protect, and provide remedies for human rights.6 These principles can provide AI executives and developers alike with a template for conducting due diligence on human rights im- pacts. They provide guidelines for how businesses should assume a higher duty of care when developing and deploying their products.7 Although a milestone in the field of business and human rights, the UN Guiding Princi- ples reflects but a starting point for the application of human rights in the tech sector. Those working directly on AI need regulation, or \\u201Chard\\u201D laws, along with technical standards, social norms, and market incentives, to effectively incorporate a respect for human rights into their business models, policies, and practices. At the same time, those working on human rights need to be actively engaged in AI governance and monitoring. When necessary, they should be ready to invoke a human rights frame- work to challenge how AI is developed and deployed by business or government. Civil society and AI developers should work together to help assess risk areas and antici- pate the needs of vulnerable groups. Only when stakeholders are working across silos to safeguard against harms can AI systems avoid human rights abuses and advance the enjoyment of human rights. This report is intended as a resource for anyone working in the field of AI and gover- nance. Anywhere that AI is being researched, developed, or deployed, a human rights frame can identify, anticipate, and minimize an important class of risks and harms. This DATA & SOCIETY 7 GOVERNING ARTIFICIAL INTELLIGENCE work is also intended for those in the human rights field, outlining why they should be concerned about the present and near-term impacts of AI. What follows translates between these fields by reframing the societal impact of AI systems through the lens of human rights. For those seeking to govern AI \\u2013 from governments looking to craft regulation to companies looking to self-regulate \\u2013 this document offers a perspective based on established human rights accountability and norms. The field of human rights has lim- itations and will certainly not address all the ethical issues arising from AI. Yet it offers a strong value proposition: an approach to AI governance that upholds human dignity based on international human rights law. The first part of this report, \\u201CBridging AI and Human Rights,\\u201D connects the entry points between AI and human rights for governance discussions. Next, \\u201CA Human Rights Frame for AI Risks and Harms\\u201D reviews a number of current AI risks and harms from a human rights perspective, describing how such rights can be applied. Part three, \\u201CStakeholder Overview,\\u201D catalogues the current state of play among stakeholders active in this space, with examples of progress and challenges. Finally, the conclusion discusses the limitations and presents several recommendations for incorporating a human rights approach for AI governance. BRIDGING AI AND HUMAN RIGHTS Human rights have only appeared at the periphery of our prominent AI debates.8 Both AI and human rights are highly technical fields; to fully digest either would require far more of an exegesis than can be attempted in this report. Instead, we shall draw on basic entry points from both fields to inform AI governance discussions. Discussions about AI can be fragmented; some people speak of AI colloquially in the popular press or in tech marketing materials, while others speak of concrete methods in scientific proceedings.9 Moreover, the nuances of terminology and the speed at which the field is moving can make cross-disciplinary discussions difficult to DATA & SOCIETY 8 GOVERNING ARTIFICIAL INTELLIGENCE have. When considering social and policy implications, it is useful to think of \\u201CAI\\u201D as a catchphrase for a cluster of technologies embedded in social systems. This includes machine learning, natural language processing, computer vision, neural networks, deep learning, big data analytics, predictive models, algorithms, and robotics\\u2014all of which are intrinsically situated in the social contexts where they are developed and deployed. While some areas of AI remain only theoretical, others, such as machine learning, are already having an impact in real-world contexts.10 Machine learning systems process large amounts of historical training data and \\u201Clearn\\u201D from these examples to detect patterns that can be useful in decision-making.11 All machine learning algorithms contain some level of statistical bias, which produces incorrect decisions some of the time.12 However, if the historical data are incomplete or are not representative of a specified population, these biases can scale quickly and inexplicably across AI systems. Such systems can further entrench discriminatory outcomes in people\\u2019s lives. How far should we as a society allow machine learning systems to influence human decision-making or even make decisions on their own? These concerns are at the heart of AI debates.13 While these questions have yet to be answered, the fact is that today, automated systems are making predictions about human behavior and producing decisions and recommendations that are impacting people in everyday life. These systems are increasingly becoming embedded in a number of social contexts, from policing and judicial sentencing to medicine and finance. We do not know the unintentional impacts or unforeseen consequences of current or future AI systems. As this uncertainty has brought urgent calls to govern AI, we can now turn to the value of human rights. The field of human rights can be complex for nonexperts. For the purposes of this report, we anchor international human rights law in the drafting and implementation of the Universal Declaration on Human Rights (UDHR) by the United Nations in 1948. The UDHR\\u2019s aspirational language established that human rights were grounded in a respect for all individuals that derived from our equal status as bearers of inherent human dignity. This was a response to the \\u201Cdisregard and contempt for human rights,\\u201D14 which precipitated two world wars and the Holocaust. Human dignity and fundamen- tal rights are not tied to country citizenship, legal regime, or socioeconomic position. These rights are universal in the sense that they apply to everyone, everywhere, which provides a frame for discussing global AI impact and governance. Over the last 70 years, human rights proponents have developed the principles of the UDHR into a body of international human rights law that includes nine major human rights treaties; regional rights instruments in the Americas, Africa, and Europe; incor- poration in state constitutions and national laws; and customary and case law.15 Yet because of a divergence in political ideologies and claims to sovereignty, governments enforce international human rights law to wildly varying degrees.16 Thus, a human rights framework has emerged to monitor, promote, and protect human rights. This involves the further development of international human rights law and the interaction DATA & SOCIETY 9 GOVERNING ARTIFICIAL INTELLIGENCE of a diverse network of actors in the UN system, nation-states, international organiza- tions, NGOs, civil society, the private sector, academia, and advocates at the local or individual level. Those looking for first principles to ground AI governance can use the language of human rights. For example, one of the most hotly debated topics in AI is discrimina- tory algorithms and systems. This includes empirical research on facial recognition systems that cannot \\u201Csee\\u201D people, particularly women, with darker skin due to a lack of adequate training data or to faulty models and therefore reproduce culturally en- grained biases against people of color.17 Human rights principles of nondiscrimination have been propagated through a multitude of UN treaties, national laws, UN commen- tary, academic interpretation, and other policies and guidelines. This body of work offers not only a distinct value commitment but also a global perspective on how to identify the impact of discrimination. Equality and nondiscrimination are foundational to practically every major human rights instrument that has emerged from debate and input from representatives from the world\\u2019s nations. The development of human rights has its own controversies and politics, but over the last 70 years, international human rights have come to represent shared global values. Those working on technology policy are faced with the difficult task of deciding what standards, values, or norms to apply in different social contexts. They need to balance the tradeoffs of developing or deploying technologies. They need to understand the potential misuses and abuses, unintended consequences, biases in sociotechnical systems, and even the costs of not deploying a tool when it may help someone in need. Human rights provide those working on AI with a basis for understanding why governing systems \\u2013 from technical standards to policy \\u2013 should address values like nondiscrimination in the first place. This is important for tech companies whose prod- ucts will be used across national borders where laws and values vary. While it is outside the present scope of this report, an area that demands more foun- dational work concerns pathways for human rights accountability and remedy when AI harms become manifest. A purely legal, regulatory, or compliance framework would lag behind the velocity of change associated with emerging AI technologies. Thus, other components of the human rights framework, such as UN special rapporteurs, independent investigators, and monitors from civil society are crucial for calling attention to AI risks and harms. As scholars Christiaan Van Veen of New York Univer- sity (NYU) and Corinne Cath of Oxford Internet Institute state, \\u201CHuman rights, as a language and legal framework, is itself a source of power because human rights carry significant moral legitimacy and the reputational cost of being perceived as a human rights violator can be very high.\\u201D18 Thus, human rights can provide the link between an AI system\\u2019s negative social impact on even one individual in places like Myanmar and the most powerful companies in Silicon Valley.\"));\n}","map":{"version":3,"sources":["/Users/Student_M7/Desktop/SE/myGitHub/industry-reports/industry-reports/src/components/Home.js"],"names":["React","Home"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AAEA,eAAe,SAASC,IAAT,GAAgB;AAC3B,sBACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAII;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mgdAJJ,CADJ;AA8PH","sourcesContent":["import React from 'react'\n\nexport default function Home() {\n    return (\n        <div>\n\n        \n\n            <p>\n            INTRODUCTION\n\nCan international human rights help guide and govern artificial intelligence (AI)?\nAccording to the global ethics initiative of the Institute of Electrical and Electronics\nEngineers (IEEE), the largest organization of technical professionals, the answer is\nclear. The IEEE’s 2017 report on ethically aligned design for AI lists as its first principle\nthat AI design should not infringe upon international human rights.1\n\nYet some AI sys-\ntems are already infringing on such rights. For instance, in March 2018, human rights\n\ninvestigators from the United Nations (UN) found that Facebook – and its algorithmi-\ncally driven news feed – exacerbated the circulation of hate speech and incitement to\n\nviolence in Myanmar.2 During a US Congressional hearing in April 2018, Senator Pat-\nrick Leahy questioned CEO Mark Zuckerberg about the failure of Facebook’s AI for\n\ncontent detection in the face of possible genocide against Myanmar’s Rohingya ethic\nminority. While Zuckerberg initially told senators that more advanced AI tools would\nhelp solve the problem, he later conceded to investors that Facebook’s AI systems will\nbe unable to detect “hate” in local contexts with reasonable accuracy anytime soon.3\nJust a month after Zuckerberg’s hearing, the UN’s International Telecommunications\nUnion (ITU) hosted its second annual AI for Global Good summit in Geneva.4 For many\ninvolved in the summit, AI is not just a source of potential risks, it can bring a better\nfuture of worldwide benefits. Between these hopes and fears lies an increased sense\nof uncertainty. As stakeholders look for a North Star to guide AI development, we can\nrely on human rights to help chart the course ahead. Simply put:\nIn order for AI to benefit the common good, at the very least its\ndesign and deployment should avoid harms to fundamental human\nvalues. International human rights provide a robust and global\nformulation of those values.\nIn bridging AI and human rights, what’s at stake is human dignity.* As an international\nframework, human rights law is intended to establish global principles (“norms”) and\nmechanisms of accountability for the treatment of individuals. As such, a rights-based\n* The definition of human dignity is contested and its normative value is debated in an extensive literature that\nis outside the scope of this report. For the present purposes, the term human dignity gestures towards its\nusage in Western moral philosophy, such as Kant’s notions of dignity linked to human autonomy and agency,\nwhile acknowledging that dignity has been linked to traditions such as Eastern philosophy as well. This report’s\nusage of human dignity also evokes the United Nation’s charter, the Universal Declaration on Human Rights,\nand the major rights treaties, which link fundamental human rights, the dignity and worth of the human person,\nand the equal rights of men and women. The interactions between humans and AI may further challenge or\nrefine the concept of human dignity, which is an important topic for future work.\n\nDATA & SOCIETY 6\n\nGOVERNING ARTIFICIAL INTELLIGENCE\n\napproach provides actors developing AI with the aspirational and normative guid-\nance to uphold human dignity and the inherent worth of every individual, regardless\n\nof country or jurisdiction. Implementing human rights can help identify and anticipate\n\nsome of AI’s worst social harms and guide those developing technical and policy safe-\nguards to promote positive uses. Those working on AI accountability can activate the\n\ninternational system of human rights practice – including binding treaties, UN investi-\ngations, and advocacy initiatives – to monitor social impacts and establish processes\n\nof redress. Importantly, advocates can use human rights to focus attention on power\nrelationships and inequalities that impact vulnerable or marginalized groups around\nthe globe.\n\nIMPLEMENTING HUMAN RIGHTS CAN HELP\nIDENTIFY AND ANTICIPATE SOME OF AI’S WORST\nSOCIAL HARMS AND GUIDE THOSE DEVELOPING\nTECHNICAL AND POLICY SAFEGUARDS TO\nPROMOTE POSITIVE USES.\n\nThose working on AI commercially might wonder why they should care about human\n\nrights. Increasingly, stakeholders are holding the private sector responsible for up-\nholding rights.5 In 2011, the UN released a landmark document – The Guiding Princi-\nples on Business and Human Rights – that calls on industry to respect, protect, and\n\nprovide remedies for human rights.6 These principles can provide AI executives and\n\ndevelopers alike with a template for conducting due diligence on human rights im-\npacts. They provide guidelines for how businesses should assume a higher duty of\n\ncare when developing and deploying their products.7\n\nAlthough a milestone in the field of business and human rights, the UN Guiding Princi-\nples reflects but a starting point for the application of human rights in the tech sector.\n\nThose working directly on AI need regulation, or “hard” laws, along with technical\nstandards, social norms, and market incentives, to effectively incorporate a respect\nfor human rights into their business models, policies, and practices. At the same time,\nthose working on human rights need to be actively engaged in AI governance and\n\nmonitoring. When necessary, they should be ready to invoke a human rights frame-\nwork to challenge how AI is developed and deployed by business or government. Civil\n\nsociety and AI developers should work together to help assess risk areas and antici-\npate the needs of vulnerable groups. Only when stakeholders are working across silos\n\nto safeguard against harms can AI systems avoid human rights abuses and advance\nthe enjoyment of human rights.\n\nThis report is intended as a resource for anyone working in the field of AI and gover-\nnance. Anywhere that AI is being researched, developed, or deployed, a human rights\n\nframe can identify, anticipate, and minimize an important class of risks and harms. This\n\nDATA & SOCIETY 7\n\nGOVERNING ARTIFICIAL INTELLIGENCE\n\nwork is also intended for those in the human rights field, outlining why they should be\nconcerned about the present and near-term impacts of AI. What follows translates\nbetween these fields by reframing the societal impact of AI systems through the lens\nof human rights.\nFor those seeking to govern AI – from governments looking to craft regulation to\ncompanies looking to self-regulate – this document offers a perspective based on\n\nestablished human rights accountability and norms. The field of human rights has lim-\nitations and will certainly not address all the ethical issues arising from AI. Yet it offers\n\na strong value proposition: an approach to AI governance that upholds human dignity\nbased on international human rights law.\nThe first part of this report, “Bridging AI and Human Rights,” connects the entry points\nbetween AI and human rights for governance discussions. Next, “A Human Rights\nFrame for AI Risks and Harms” reviews a number of current AI risks and harms from\na human rights perspective, describing how such rights can be applied. Part three,\n“Stakeholder Overview,” catalogues the current state of play among stakeholders\nactive in this space, with examples of progress and challenges. Finally, the conclusion\ndiscusses the limitations and presents several recommendations for incorporating a\nhuman rights approach for AI governance.\n\nBRIDGING AI AND HUMAN RIGHTS\n\nHuman rights have only appeared at the periphery of our prominent AI debates.8 Both\nAI and human rights are highly technical fields; to fully digest either would require far\nmore of an exegesis than can be attempted in this report. Instead, we shall draw on\nbasic entry points from both fields to inform AI governance discussions.\nDiscussions about AI can be fragmented; some people speak of AI colloquially in\nthe popular press or in tech marketing materials, while others speak of concrete\nmethods in scientific proceedings.9 Moreover, the nuances of terminology and the\nspeed at which the field is moving can make cross-disciplinary discussions difficult to\n\nDATA & SOCIETY 8\n\nGOVERNING ARTIFICIAL INTELLIGENCE\n\nhave. When considering social and policy implications, it is useful to think of “AI” as a\ncatchphrase for a cluster of technologies embedded in social systems. This includes\nmachine learning, natural language processing, computer vision, neural networks,\ndeep learning, big data analytics, predictive models, algorithms, and robotics—all of\nwhich are intrinsically situated in the social contexts where they are developed and\ndeployed.\nWhile some areas of AI remain only theoretical, others, such as machine learning, are\nalready having an impact in real-world contexts.10 Machine learning systems process\nlarge amounts of historical training data and “learn” from these examples to detect\npatterns that can be useful in decision-making.11 All machine learning algorithms\ncontain some level of statistical bias, which produces incorrect decisions some of\nthe time.12 However, if the historical data are incomplete or are not representative of a specified population, these biases can scale quickly and inexplicably across AI systems. Such systems can further entrench discriminatory outcomes in people’s lives.\n\nHow far should we as a society allow machine learning systems to influence human\ndecision-making or even make decisions on their own? These concerns are at the\nheart of AI debates.13\nWhile these questions have yet to be answered, the fact is that today, automated\nsystems are making predictions about human behavior and producing decisions and\nrecommendations that are impacting people in everyday life. These systems are\nincreasingly becoming embedded in a number of social contexts, from policing and\njudicial sentencing to medicine and finance. We do not know the unintentional impacts\nor unforeseen consequences of current or future AI systems. As this uncertainty has\nbrought urgent calls to govern AI, we can now turn to the value of human rights.\nThe field of human rights can be complex for nonexperts. For the purposes of this\nreport, we anchor international human rights law in the drafting and implementation\nof the Universal Declaration on Human Rights (UDHR) by the United Nations in 1948.\nThe UDHR’s aspirational language established that human rights were grounded in a\n\nrespect for all individuals that derived from our equal status as bearers of inherent human dignity. This was a response to the “disregard and contempt for human rights,”14\n\nwhich precipitated two world wars and the Holocaust. Human dignity and fundamen-\ntal rights are not tied to country citizenship, legal regime, or socioeconomic position.\n\nThese rights are universal in the sense that they apply to everyone, everywhere, which\nprovides a frame for discussing global AI impact and governance.\nOver the last 70 years, human rights proponents have developed the principles of the\nUDHR into a body of international human rights law that includes nine major human\n\nrights treaties; regional rights instruments in the Americas, Africa, and Europe; incor-\nporation in state constitutions and national laws; and customary and case law.15 Yet\n\nbecause of a divergence in political ideologies and claims to sovereignty, governments\nenforce international human rights law to wildly varying degrees.16 Thus, a human\nrights framework has emerged to monitor, promote, and protect human rights. This\ninvolves the further development of international human rights law and the interaction\n\nDATA & SOCIETY 9\n\nGOVERNING ARTIFICIAL INTELLIGENCE\n\nof a diverse network of actors in the UN system, nation-states, international organiza-\ntions, NGOs, civil society, the private sector, academia, and advocates at the local or\n\nindividual level.\nThose looking for first principles to ground AI governance can use the language of\n\nhuman rights. For example, one of the most hotly debated topics in AI is discrimina-\ntory algorithms and systems. This includes empirical research on facial recognition\n\nsystems that cannot “see” people, particularly women, with darker skin due to a lack\n\nof adequate training data or to faulty models and therefore reproduce culturally en-\ngrained biases against people of color.17 Human rights principles of nondiscrimination\n\nhave been propagated through a multitude of UN treaties, national laws, UN commen-\ntary, academic interpretation, and other policies and guidelines. This body of work\n\noffers not only a distinct value commitment but also a global perspective on how to\nidentify the impact of discrimination. Equality and nondiscrimination are foundational\nto practically every major human rights instrument that has emerged from debate and\ninput from representatives from the world’s nations. The development of human rights\nhas its own controversies and politics, but over the last 70 years, international human\nrights have come to represent shared global values.\nThose working on technology policy are faced with the difficult task of deciding what\nstandards, values, or norms to apply in different social contexts. They need to balance\nthe tradeoffs of developing or deploying technologies. They need to understand the\npotential misuses and abuses, unintended consequences, biases in sociotechnical\nsystems, and even the costs of not deploying a tool when it may help someone in\nneed. Human rights provide those working on AI with a basis for understanding why\ngoverning systems – from technical standards to policy – should address values like\n\nnondiscrimination in the first place. This is important for tech companies whose prod-\nucts will be used across national borders where laws and values vary.\n\nWhile it is outside the present scope of this report, an area that demands more foun-\ndational work concerns pathways for human rights accountability and remedy when\n\nAI harms become manifest. A purely legal, regulatory, or compliance framework would\nlag behind the velocity of change associated with emerging AI technologies. Thus,\nother components of the human rights framework, such as UN special rapporteurs,\nindependent investigators, and monitors from civil society are crucial for calling\n\nattention to AI risks and harms. As scholars Christiaan Van Veen of New York Univer-\nsity (NYU) and Corinne Cath of Oxford Internet Institute state, “Human rights, as a\n\nlanguage and legal framework, is itself a source of power because human rights carry\nsignificant moral legitimacy and the reputational cost of being perceived as a human\nrights violator can be very high.”18 Thus, human rights can provide the link between an\nAI system’s negative social impact on even one individual in places like Myanmar and\nthe most powerful companies in Silicon Valley.\n\n             </p>\n            \n        </div>\n    )\n}\n"]},"metadata":{},"sourceType":"module"}